---
title: Operating an On-Demand Broker
owner: London Services Enablement
---

This topic provides information for Ops Manager or BOSH operators. These roles may be combined or separate.

## <a id="operator"></a>Introduction for Operators

### <a id="deliverables"></a> Operator Deliverables

The operator is responsible for the following:

* Request appropriate networking rules for on-demand service tiles.
* Configure the BOSH Director.
* Upload the required releases for the broker deployment and service instance deployments.
* Write a broker manifest. See [Write a Broker Manifest](#broker-manifest).
* Manage brokers and service plans. See [Broker and Service Management](./management.html).
* Provide any necessary documentation.

For a list of deliverables provided by the Service Author, see [Creating Service Author Deliverables](./creating.html).


### <a id="about-cli"></a>About the BOSH CLI

You use the BOSH CLI to perform many of the operator tasks.
The BOSH CLI is available in two major versions, v1 and v2. Pivotal recommends that you use the BOSH CLI v2 when possible. 
Instructions in this document are shown only for the v2 version.

The table below shows the version of the BOSH CLI supported for different versions of PCF.

<table class="nice">
    <th>PCF Version</th>
    <th>BOSH CLI Version</th>
    <tr>
        <td>1.10</td>
        <td>CLI v1</td>
    </tr>
    <tr>
        <td>1.11</td>
        <td>CLI v1 or CLI v2 (Pivotal recommends CLI v2)</td>
    </tr>
    <tr>
        <td>1.12 and later</td>
        <td>CLI v2</td>
    </tr>
</table>

## <a id="networking"></a>Set Up Networking

<%= partial 'service_networks_table' %>

<br>
Regardless of the specific network layout, the operator must ensure network
rules are set up so that connections are open as described in the table below.

<table class="nice">
  <th>This component...</th>
  <th>Must communicate with...</th>
  <th>Default TCP Port</th>
  <th>Communication direction(s)</th>
  <th>Notes</th>
  <tr>
    <td><strong>ODB</strong></td>
    <td>
        <ul>
            <li><strong>BOSH Director</strong></li>
          <li><strong>BOSH UAA</strong></li>
        </ul>
    </td>
    <td>
      <ul>
        <li>25555</li>
        <li>8443</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default ports are not configurable.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service (such as RabbitMQ for PCF).
      May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for administrative tasks.
      Avoid opening general use, app-specific ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>PAS (or Elastic Runtime)</strong>
    </td>
    <td>8443</td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>Errand VMs</strong></td>
    <td>
      <ul>
        <li><strong>PAS (or Elastic Runtime) </strong></li>
        <li><strong>ODB</strong></li>
        <li><strong>Deployed Service Instances</strong></li>
      </ul>
    </td>
    <td>
      <ul>
        <li>8443</li>
        <li>8080</li>
        <li>Specific to the service. May be one or more ports.</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>BOSH Agent</strong></td>
    <td><strong>BOSH Director</strong>
    </td>
    <td>4222</td>
    <td>Two-way</td>
    <td>The BOSH Agent runs on every VM in the system, including the BOSH Director VM.
      The BOSH Agent initiates the connection with the BOSH Director.<br>
      The default port is not configurable.  </td>
  </tr>
  <tr>
    <td><strong>Deployed apps on PAS (or Elastic Runtime)</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service. May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for general use, app-specific tasks.
      Avoid opening administrative ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>PAS (or Elastic Runtime)</strong></td>
    <td><strong>ODB</strong>
    </td>
    <td>8080</td>
    <td>One-way</td>
    <td>This port may be different for individual services.
      This port may also be configurable by the operator if allowed by the
      tile developer.</td>
  </tr>
</table>

## <a id="configure-bosh"></a>Set Up Your BOSH Director

See the following topics for how to set up your BOSH Director:

* [Software Requirements](#software-reqs)
* [SSL Certificates](#ssl-certs)
* [BOSH Teams](#bosh-teams)
* [Cloud Controller](#cloud-controller)

### <a id="software-reqs"></a> Software Requirements

The On-Demand Broker requires the following:

- BOSH Director v257 or later (PCF 1.8) 
- Cloud Foundry v238 or later (PCF 1.8)

<div class="note"><strong>Notes</strong>: <ul><li>ODB does not support BOSH Windows.</li>
<li><a href="#lifecycle-errands"> Service Instance Lifecycle Errands</a> require BOSH Director v261 (PCF 1.10) or later.</li></ul></div>


### <a id="ssl-certs"></a>SSL Certificates

If ODB is configured to communicate with BOSH on the Director's public IP, you may be using a self-signed certificate unless you have a domain for your BOSH Director. 
ODB does not ignore TLS certificate validation errors by default. 

You have two options to configure certificate-based authentication between the BOSH Director and the ODB:

* Add the BOSH Director's root certificate to ODB's trusted pool in the ODB manifest as follows:

    ```yaml
    bosh:
      root_ca_cert: ROOT-CA-CERT
    ```

* Use BOSH's `trusted_certs` feature to add a self-signed CA certificate to each VM that BOSH deploys. 
For more details on how to generate and use self-signed certificates for BOSH Director and UAA, 
see the [BOSH documentation](https://bosh.io/docs/director-certs.html).

Optionally, you can configure a separate root CA certificate that is used when ODB communicates with the Cloud Foundry API (Cloud Controller).
For an example of how to add this in the manifest, see the line containing 
`CA-CERT-FOR-CLOUD-CONTROLLER` in the manifest snippet in [Core Broker Configuration](#core-broker-config) below.


### <a id="bosh-teams"></a>BOSH Teams

BOSH has a teams feature that allows you to further control how BOSH operations are available to different clients. 
Pivotal recommends using this feature to ensure that your on-demand service broker client can only modify deployments it created. 

To use this feature do the following:

1. Run the following UAA CLI (UAAC) command to create the client:

    ```
    uaac client add CLIENT-ID \
      --secret CLIENT-SECRET \
      --authorized_grant_types "refresh_token password client_credentials" \
      --authorities "bosh.teams.TEAM-NAME.admin"
    ```
    <br>
    Where:
    * `CLIENT-ID` is your client ID.
    * `CLIENT-SECRET` is your client secret.
    * `TEAM-NAME` is the name of the team authorized to modify this deployment.
    <br><br>
    For example:
    <pre class="terminal">
    uaac client add admin \
      --secret 12345679 \
      --authorized\_grant\_types "refresh\_token password client\_credentials" \
      --authorities "bosh.teams.my-team.admin"
    </pre>
    For more information about using the UAAC, see [Creating and Managing Users with the UAA CLI (UAAC)
    ](https://docs.cloudfoundry.org/uaa/uaa-user-management.html).

1. When configuring the broker's BOSH authentication, use the client ID and secret you provided above. 
The broker can then only perform BOSH operations on deployments it has created. See [Core Broker Configuration](#core-broker-config) below.

For more information about setting up and using BOSH teams, see the [BOSH documentation](https://bosh.io/docs/director-users-uaa-perms.html).

For more details on securing how ODB uses BOSH, see [Security](./security.html).

### <a id="cloud-controller"></a>Cloud Controller

ODB uses the Cloud Controller as a source of truth about service offerings, plans, and instances. 
To reach the Cloud Controller, configure ODB with either client or user credentials in the broker manifest. 
The client or user must have the following permissions:

* **If using client credentials**: As of Cloud Foundry v238, the UAA client must have the authority `cloud_controller.admin`.<br><br>
* **If using user credentials**: The user must be a Cloud Foundry admin user, that is, a member of the `scim.read` and `cloud_controller.admin` groups as a minimum.

For detailed information, see [Write a Broker Manifest](#broker-manifest) below.

## <a id="upload-releases"></a>Upload Required Releases

Upload the following releases to the BOSH Director:

* On-demand-service-broker
* Your service adapter
* Your service release(s)

## <a id="broker-manifest"></a>Write a Broker Manifest

The broker manifest contains:

* [Core Broker Configuration](#core-broker-config)
* [Service Catalog and Plan Composition](#catalog)

If you are unfamiliar with writing BOSH v2 manifests, see [Manifest v2 Schema](http://bosh.io/docs/manifest-v2.html).

For example manifests, see the following:

  * For a Redis service---[redis-example-service-adapter-release](https://github.com/pivotal-cf-experimental/redis-example-service-adapter-release/blob/master/docs/example-manifest.yml).

  * For a Kafka service---[kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml).


### <a id="core-broker-config"></a>Core Broker Configuration

Your manifest should contain one non-errand instance group that is colocated with both of the following:

* the `broker` job from on-demand-service-broker
* your service adapter job from your service adapter release

The broker is stateless and does not need a persistent disk. 
The VM type can be small: a single CPU and 1 GB of memory is sufficient in most cases.

An example snippet is shown below. This snippet uses the BOSH v2 syntax and 
makes use of global cloud config and job-level properties.

<p class="note warning"><strong>WARNING</strong>: The <code>disable_ssl_cert_verification</code>
  option is dangerous and should be set to <code>false</code> in production</strong>.</p>

```yaml
instance_groups:
  - name: broker # this can be anything
    instances: 1
    vm_type: VM-TYPE
    stemcell: STEMCELL
    networks:
      - name: NETWORK
    jobs:
      - name: SERVICE-ADAPTER-JOB-NAME
        release: SERVICE-ADAPTER-RELEASE
      - name: broker
        release: on-demand-service-broker
        properties:
          # choose a port and basic auth credentials for the broker
          port: BROKER-PORT
          username: BROKER-USERNAME
          password: BROKER-PASSWORD
          disable_ssl_cert_verification: true|false # optional, defaults to false. This should NOT set to true in production
          shutdown_timeout_in_seconds: 60 # optional, defaults to 60 seconds. This allows the broker to gracefully wait for any open requests to complete before shutting down.
          expose_operational_errors: true|false # optional, defaults to false. This allows for BOSH operational errors to be displayed for the CF user.
          enable_plan_schemas: true|false # optional, defaults to false. If set to true, plan schemas are included in the catalog, and the broker fails if the adapter does not implement generate-plan-schemas.
          cf:
            url: CF-API-URL
            root_ca_cert: CA-CERT-FOR-CLOUD-CONTROLLER # optional, see SSL certificates
            authentication: # either client_credentials or user_credentials, not both as shown
              url: CF-UAA-URL
              client_credentials:
                client_id: UAA-CLIENT-ID # with cloud_controller.admin authority and client_credentials in the authorized_grant_type
                secret: UAA-CLIENT-SECRET
              user_credentials:
                username: CF-ADMIN-USERNAME # in the cloud_controller.admin and scim.read groups
                password: CF-ADMIN-PASSWORD
          bosh:
            url: DIRECTOR-URL
            root_ca_cert: CA-CERT-FOR-BOSH-DIRECTOR-AND-ASSOCIATED-UAA # optional, see SSL certificates
            authentication: # either basic or uaa, not both as shown
              basic:
                username: BOSH-USERNAME
                password: BOSH-PASSWORD
              uaa:
                client_id: BOSH-CLIENT-ID
                client_secret: BOSH-CLIENT-SECRET
          service_adapter:
            path: PATH-TO-SERVICE-ADAPTER-BINARY # optional, provided by the Service Author. Defaults to /var/vcap/packages/odb-service-adapter/bin/service-adapter

          # There are more broker properties that are discussed below
```


### <a id="catalog"></a>Service Catalog and Plan Composition

The operator must provide the following in the manifest. To do so, use the starter [snippet](#starter-snippet) 
provided below in the broker job properties section of the manifest.

1. Supply each release job specified by the Service Author exactly once. You can include releases that provide many jobs, as long as each required job is provided by exactly one release.

1. Supply one stemcell that is used on each VM in the service deployments. ODB does not currently support service instance deployments that use a different stemcell for different instance groups.

1. Use exact versions for releases and stemcells. The use of `latest` and floating stemcells are not supported.

1. Create Cloud Foundry [service metadata](https://docs.pivotal.io/pivotalcf/services/catalog-metadata.html#services-metadata-fields) in the catalog for the service offering. 
  You can use other arbitrary field names as needed in addition to the OSBAPI recommended fields.
  This metadata will be aggregated in the Cloud Foundry marketplace and displayed in Apps Manager and the cf CLI.

1. Compose plans. In ODB, service authors do not define plans but instead expose plan properties. 
The operator's role is to compose combinations of these properties, along with IAAS resources and catalog metadata into as many plans as they wish.
  1. Create Cloud Foundry [plan metadata](https://docs.pivotal.io/pivotalcf/services/catalog-metadata.html#plan-metadata-fields) in the service catalog for each plan. 
    You can use other arbitrary field names as needed in addition to the OSBAPI recommended fields.<br><br>
  1. Provide resource mapping for each instance group specified by the Service Author for each plan.
     The resource values must correspond to valid resource definitions in the BOSH Director's global cloud config.<br><br>
     In some cases Service Authors recommend resource configuration: for example, 
     in single-node Redis deployments, an instance count greater than 1 does not make sense.
     Here the operator can configure the deployment to span multiple availability zones 
     by using the [BOSH multi-az feature](https://bosh.io/docs/azs.html). <br><br>
     In some cases, service authors will provide errands for the service release. 
     You can add an instance group of type `errand` by setting the `lifecycle` field. 
     For an example see `register-broker` in the [example kafka deployment](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml).<br><br>
  1. Provide values for plan properties.
     Plan properties are key-value pairs defined by the Service Author.<br><br> 
     Some examples of these could be to include a boolean to enable disk persistence for Redis, 
     or a list of strings representing RabbitMQ plugins to load.<br><br>
     The Service Author should document whether these properties are mandatory or optional, 
     whether the use of one property precludes the use of another, and whether certain properties affect recommended instance group to resource mappings.<br><br>
     You can also specify properties at the service offering level, where they will be applied to every plan. 
     If there is a conflict between global and plan-level properties, the plan properties take precedence.<br><br>
  1. (Optional) Provide an  update block for each plan.<br><br>
     You may require plan-specific configuration for BOSH's update instance operation. <br><br>
     The ODB passes the plan-specific update block to the service adapter. 
     Plan-specific update blocks should have the same structure as [the update block in a BOSH manifest](https://bosh.io/docs/manifest-v2.html#update). <br><br>
     The Service Author can define a default update block to be used when a plan-specific update block is not provided, 
     if the service adapter supports configuring update blocks in the manifest.

<a id="starter-snippet"></a>Add this Snippet to your broker job properties section:

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION # Exact release version
      jobs: [RELEASE-JOBS-NEEDED-FOR-DEPLOYMENT-AND-LIFECYCLE-ERRANDS] # Service Author will specify list of jobs required
  stemcell: # every instance group in the service deployment has the same stemcell
    os: SERVICE-STEMCELL
    version: SERVICE-STEMCELL-VERSION # Exact stemcell version
service_catalog:
  id: CF-MARKETPLACE-ID
  service_name: CF-MARKETPLACE-SERVICE-OFFERING-NAME
  service_description: CF-MARKETPLACE-DESCRIPTION
  bindable: TRUE|FALSE
  plan_updatable: TRUE|FALSE # optional
  tags: [TAGS] # optional
  requires: [REQUIRED-PERMISSIONS] # optional
  dashboard_client: # optional
    id: DASHBOARD-OAUTH-CLIENT-ID
    secret: DASHBOARD-OAUTH-CLIENT-SECRET
    redirect_uri: DASHBOARD-OAUTH-REDIRECT-URI
  metadata: # optional
    display_name: DISPLAY-NAME
    image_url: IMAGE-URL
    long_description: LONG-DESCRIPTION
    provider_display_name: PROVIDER-DISPLAY-NAME
    documentation_url: DOCUMENTATION-URL
    support_url: SUPPORT-URL
  global_properties: {} # optional, applied to every plan.
  global_quotas: # optional
    service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances across all plans
    resource_limits: # optional - resource usage limits, determined by the 'cost' of each service instance plan
      ips: RESOURCE-LIMIT
      memory: RESOURCE-LIMIT
  plans:
    - name: CF-MARKETPLACE-PLAN-NAME
      plan_id: CF-MARKETPLACE-PLAN-ID
      description: CF-MARKETPLACE-DESCRIPTION
      cf_service_access: ENABLE|DISABLE|MANUAL # optional, enable by default.
      bindable: TRUE|FALSE # optional. If specified, this takes precedence over the bindable attribute of the service
      metadata: # optional
        display_name: DISPLAY-NAME
        bullets: [BULLET1, BULLET2]
        costs:
          - amount:
              CURRENCY-CODE-STRING: CURRENCY-AMOUNT-FLOAT
            unit: FREQUENCY-OF-COST
      resource_costs: # optional – the 'cost' of each instance in terms of resource quotas
        memory: AMOUNT-OF-RESOURCE-IN-THIS-PLAN
      quotas: # optional
        service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances for this plan
        resource_limits: # optional - resource usage limits for this plan
          memory: RESOURCE-LIMIT
      instance_groups: # resource mapping for the instance groups defined by the Service Author
        - name: SERVICE-AUTHOR-PROVIDED-INSTANCE-GROUP-NAME
          vm_type: VM-TYPE
          vm_extensions: [VM-EXTENSIONS] # optional
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
          persistent_disk_type: DISK # optional
        - name: SERVICE-AUTHOR-PROVIDED-LIFECYCLE-ERRAND-NAME # optional
          lifecycle: errand
          vm_type: VM-TYPE
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
      properties: {} # valid property key-value pairs are defined by the Service Author
      update: # optional
        canaries: 1 # optional
        max_in_flight: 2  # required
        canary_watch_time: 1000-30000 # required
        update_watch_time: 1000-30000 # required
        serial: true # optional
      lifecycle_errands: #optional
        post_deploy:
          name: ERRAND-NAME #optional
          instances: [INSTANCE-NAME] #optional, for co-locating errand
        pre_delete: ERRAND-NAME #optional
```

## <a id="secure-binding"></a>Enabling Secure Binding

Optionally, ODB allows the secure storage of service credentials in runtime CredHub.
To use this feature, you must connect to CredHub v1.6.x or later.
This may be provided as part of your Cloud Foundry deployment.

The address for CredHub is a local domain name, therefore your instance group
requires access to the local DNS provider.
Currently, most Cloud Foundry deployments use Consul as a DNS provider.

To enable secure bindings, you must do the following in the ODB manifest:

* Consume the CredHub link.
* Include the secure binding credentials section with `secure_binding_credentials.enabled`
set to `true`.
* If you use Consul as a DNS provider, you might want to consume Consul links to enable DNS.

If the Secure Binding Credentials section is omitted, or if
`secure_binding_credentials.enabled` is `false`, service credentials,
as opposed to a secure reference, are passed through the network to the
application's container.

An example manifest snippet, assuming you are connecting to a Cloud
Foundry deployment, is shown below:

```
instance_groups:
- name: broker
  ...
  jobs:
  - name: broker
    consumes:
      credhub:
        from: credhub
        deployment: cf
    properties:
      ...
      secure_binding_credentials:
        enabled: true
        authentication:
          uaa:
            client_id: CREDHUB-CLIENT-ID
            client_secret: CREDHUB-CLIENT-SECRET
            ca_cert: ((cf.uaa.ca_cert))
  - name: consul_agent
    release: consul
    consumes:
      consul: nil
      consul_client:
        from: consul_client_link
        deployment: cf
      consul_common:
        from: consul_common_link
        deployment: cf
      consul_server: nil
```

<p class="note"><strong>Note</strong>: You must set up a new CredHub client in
Cloud Foundry UAA with <code>credhub.write</code> and <code>credhub.read</code>
in its list of authorities.</p>



### <a id="credhub-key-format"></a> How credentials are stored on CredHub

The credentials for a given Service Binding are stored with the following format:

```
/C/:SERVICE-GUID/:SERVICE-INSTANCE-GUID/:BINDING-GUID/CREDENTIALS
```

The plain-text credentials are stored in CredHub under this key, and the key is
available under the `VCAP_SERVICES` environment variable for the application.

## <a id="plan-schemas"></a>Enable Plan Schemas

As of [OSBAPI Spec v2.13](https://github.com/openservicebrokerapi/servicebroker/blob/v2.13/spec.md#changes-since-v212),
ODB supports plan schemas in the service catalog.

To use
this feature, do the following:

1. Set the `enable_plan_schemas` flag to `true`.
   The default is `false`.
2. Ensure that the service adapter implements the command `generate-plan-schemas`,
otherwise the broker fails to deploy.
For more information, see [generate-plan-schemas](creating.html#generate-plan-schemas).

During provision, bind, and update, the broker validates the incoming configuration
parameters against the schema and produces an error if the parameters do not conform.

## <a id="route"></a>Route Registration

You can optionally colocate the `route_registrar` job from the [routing release](http://bosh.io/releases/github.com/cloudfoundry-incubator/cf-routing-release?all=1) with the on-demand-service-broker, in order to:

1. Load balance multiple instances of ODB using Cloud Foundry's router
1. Access ODB from the public internet

To do this, upload the release to your BOSH Director and [configure the job properties](http://bosh.io/jobs/route_registrar?source=github.com/cloudfoundry-incubator/cf-routing-release&version=0.136.0), replacing the version in that docs URL query string as appropriate.

Remember to set the `broker_uri` property in the [register-broker errand](#register-broker) if you configure a route.

## <a id="service-instance-quotas"></a>Service Instance Quotas

ODB offers global and plan level service quotas to set service instance limits.

- **Global quotas**---restrict the number of service instances across all plans.
- **Plan quotas**---restrict the number of service instances for a given plan.

When creating a service instance, ODB checks the global service instance limit.
If this limit has not been reached, ODB checks the plan service instance limit.

<p class="note"><strong>Note</strong>: These limits do not include orphans. See <a href="#listing-orphans">List Orphan Deployments</a>
   and <a href="#orphan-deployments">Delete Orphaned Deployments</a>.</p>

## <a id="service-resource-quotas"></a>Service Resource Quotas

ODB offers resource quotas allow you to limit resources more effectively when combining 
plans that consume different amounts of resources. You can set these quotas for service resources: 

- **Global resource quotas**---limit how much of a certain resource is consumed across all plans.
ODB allows new instances to be created until their total resources reach the global quota.
- **Plan resource quotas**---limit how much of a certain resource is consumed by a specific plan.

When creating a service instance, ODB checks the global resource limit.
If this limit has not been reached, ODB checks the plan resource limit.

<p class="note"><strong>Note</strong>: These limits do not include orphans. See <a href="#listing-orphans">List Orphan Deployments</a>
   and <a href="#orphan-deployments">Delete Orphaned Deployments</a>.</p>

To set resource quotas, do the following:

1. In the manifest, define the resources you want to limit. 
Use strings to define the resource, for example `ips`, `memory`, or `disk`.
Then decide the total amount allowed for each resource. <br><br>
For example: <br><br>
    - **Global resource quota**

    ```
    service_deployment:
      ...
      global_quotas:
        resource_limits:
          ips: 50
          memory: 150
          ...
    ```
    - **Resource quota for each plan**

    ```
    plans:
    - name: my-plan
      quotas:
        resource_limits:
          memory: 25
    ```

1. In order to use resource quotas, for each plan, you must define the `resource_costs` 
used by one service instance. <br><br>
For example:

    ```
    plans:
    - name: my-plan
      quotas:
        resource_limits:
            memory: 25
      resource_costs:
        memory: 5 # the key is string-matched against keys in the global and plan-level resource quotas
    ```

## <a id="broker-metrics"></a>Broker Metrics

The ODB BOSH release contains a metrics job, that can be used to emit metrics when colocated with [service metrics](https://network.pivotal.io/products/service-metrics-sdk/). You must include the [loggregator release](https://github.com/cloudfoundry/loggregator) in order to do this.

Add the following jobs to the broker instance group:

```yaml
- name: service-metrics
  release: service-metrics
  properties:
    service_metrics:
      execution_interval_seconds: INTERVAL-BETWEEN-SUCCESSIVE-METRICS-COLLECTIONS
      origin: ORIGIN-TAG-FOR-METRICS
      monit_dependencies: [broker] # hardcode this
      ....snip....
      #Add Loggregator configuration here: see examples @ https://github.com/pivotal-cf/service-metrics-release/blob/master/manifests
      ....snip....
- name: service-metrics-adapter
  release: ODB-RELEASE
```

An example of how the service metrics can be configured for an on-demand-broker deployment can be seen in the [kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml#L106) manifest.

Pivotal have tested this example configuration with loggregator v58 and service-metrics v1.5.0.

For more information about service metrics, see [Service Metrics for Pivotal Cloud Foundry](http://docs.pivotal.io/service-metrics).

## <a id="startup-checks"></a>Broker Startup Checks

The following startup checks occur:

* Verify that the CF and BOSH versions satisfy the minimum requirement. **Note**: If your service offering includes lifecycle errands, the minimum requirement for BOSH will be higher. See [Set Up Your BOSH Director](#configure-bosh). If your system does not meet minimum requirements, you will see an insufficient version error. For example:
```
CF API error: Cloud Foundry API version is insufficient, ODB requires CF v238+.
```
* Verify that, for the service offering, no plan IDs have changed for plans that have existing service instances. If there are instances, you will see following error:
```
You cannot change the plan_id of a plan that has existing service instances.
```

## <a id="broker-stop"></a>Broker Shutdown

The broker will try and wait for any incomplete https requests to complete before shutting down. This allows us to reduce the risk of leaving orphan deployments in the event that the BOSH Director does not respond to our initial `bosh deploy` request. The `broker.shutdown_timeout` property determines how long the broker will wait before it is forced to shutdown. The default is 60 seconds, but can be configured in the manifest. Please refer to how to <a href="#broker-manifest">Write a Broker Manifest</a>

## <a id="lifecycle-errands"></a>Service Instance Lifecycle Errands

 <p class="note"><strong>Note</strong>: This feature requires BOSH Director v261 or later.</p>

Service Instance lifecycle errands allow additional short lived jobs to be run as part of service instance deployment. A deployment is only considered successful if all lifecycle errands successfully exit.

The service adapter must offer the errands as part of the service instance deployment.

ODB supports the following lifecycle errands:

- `post_deploy` - Runs after the creation or updating of a service instance. An example use case is running a health check to ensure the service instance is functioning. See the workflow [here](./concepts.html#post-deploy)
- `pre_delete` - Runs before the deletion of a service instance. An example use case is cleaning up data prior to a service shutdown. See the workflow [here](./concepts.html#pre-delete)

Service Instance lifecycle errands are configured on a per-plan basis. To enable lifecycle errands, the errand job must be:

- Added to the service instance deployment.
- Added to the plan's instance groups.
- Set in the plan's lifecycle errands configuration.

An example manifest snippet configuring lifecycle errands for a plan:

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION
      jobs:
      - SERVICE-RELEASE-JOB
      - POST-DEPLOY-ERRAND-JOB
      - PRE-DELETE-ERRAND-JOB
service_catalog:
  plans:
      - name: CF-MARKETPLACE-PLAN-NAME
        lifecycle_errands:
          post_deploy:
          - name: POST-DEPLOY-ERRAND-JOB
          - name: ANOTHER-POST-DEPLOY-ERRAND-JOB
          pre_delete:
          - name: PRE-DELETE-ERRAND-JOB
        instance_groups:
          - name: SERVICE-RELEASE-JOB
            ...
          - name: POST-DEPLOY-ERRAND-JOB
            lifecycle: errand
            vm_type: VM-TYPE
            instances: INSTANCE-COUNT
            networks: [NETWORK]
            azs: [AZ]
          - name: ANOTHER-POST-DEPLOY-ERRAND-JOB
            lifecycle: errand
            vm_type: VM-TYPE
            instances: INSTANCE-COUNT
            networks: [NETWORK]
            azs: [AZ]
          - name: PRE-DELETE-ERRAND-JOB
            lifecycle: errand
            vm_type: VM-TYPE
            instances: INSTANCE-COUNT
            networks: [NETWORK]
            azs: [AZ]
```

Changing a plan's lifecycle errands configuration while an existing deployment is in progress is not supported.
Lifecycle errands will not be run.

### <a id="colocated-errands"></a> Colocated Errands

<p class="note"><strong>Note</strong>: This feature requires BOSH Director v263 or later.</p>

Both `post-deploy` and `pre-delete` errands can be run as colocated errands.
Colocated errands run on an existing service instance group, avoiding additional resource allocation.
For more information see the [BOSH documentation](https://bosh.io/docs/errands.html).

Similarly to the other lifecycle errands, colocated errands are deployed on a per-plan basis. Currently the ODB supports only colocated post-deploy or pre-delete errands.
In order to enable a colocated errand, the errand job must be:

- Added to the service instance deployment (1).
- Set in the plan's lifecycle errands configuration (2).
- Set the instances where the errand should run on (3).

An example manifest for colocated post-deploy errand looks like the following:

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION
      jobs:
      - SERVICE-RELEASE-JOB
      - COLOCATED-POST-DEPLOY-ERRAND-JOB # (1)
service_catalog:
  plans:
      - name: CF-MARKETPLACE-PLAN-NAME
        lifecycle_errands:
          post_deploy:
            - name: COLOCATED-POST-DEPLOY-ERRAND-JOB # (2)
              instances: [SERVICE-RELEASE-JOB/0] # (3)
            - name: NON-COLOCATED-POST-DEPLOY-ERRAND
        instance_groups:
          - name: NON-COLOCATED-POST-DEPLOY-ERRAND
            ...
          - name: SERVICE-RELEASE-JOB
            ...
```

